{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load and normalize CIFAR10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#define our device as the first visible cuda device if we have CUDA available\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Define a Convolutional Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)  # 输出: 32x32x16\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # 输出: 16x16x16\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # 输出: 16x16x32\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # 输出: 8x8x32\n",
    "        self.conv3 = nn.Conv2d(32, 16, kernel_size=3, padding=1)  # 输出: 8x8x16\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # 输出: 4x4x16 => 展平后是 4*4*16 = 256 个元素\n",
    "        self.conv4 = nn.Conv2d(16, 64, kernel_size=3, padding=1)  # 输出: 4x4x64\n",
    "        # 展平后是 4*4*64 = 1024 个元素\n",
    "\n",
    "        self.fc1 = nn.Linear(1024,243)\n",
    "        self.fc2 = nn.Linear(243, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = F.relu(self.conv4(x))\n",
    "\n",
    "        # 打印形状，确认展平后是1024\n",
    "        # print(f\"Shape before flatten: {x.shape}\")\n",
    "        \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net().to(device) #use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zijian\\.conda\\envs\\d2l\\Lib\\site-packages\\torchtt-0.3-py3.11.egg\\torchtt\\_dmrg.py:19: UserWarning: \u001b[33m\n",
      "C++ implementation not available. Using pure Python.\n",
      "\u001b[0m\n",
      "c:\\Users\\zijian\\.conda\\envs\\d2l\\Lib\\site-packages\\torchtt-0.3-py3.11.egg\\torchtt\\_amen.py:21: UserWarning: \u001b[33m\n",
      "C++ implementation not available. Using pure Python.\n",
      "\u001b[0m\n",
      "c:\\Users\\zijian\\.conda\\envs\\d2l\\Lib\\site-packages\\torchtt-0.3-py3.11.egg\\torchtt\\solvers.py:21: UserWarning: \u001b[33m\n",
      "C++ implementation not available. Using pure Python.\n",
      "\u001b[0m\n",
      "c:\\Users\\zijian\\.conda\\envs\\d2l\\Lib\\site-packages\\torchtt-0.3-py3.11.egg\\torchtt\\cpp.py:12: UserWarning: \u001b[33m\n",
      "C++ implementation not available. Using pure Python.\n",
      "\u001b[0m\n",
      "c:\\Users\\zijian\\.conda\\envs\\d2l\\Lib\\site-packages\\torchtt-0.3-py3.11.egg\\torchtt\\__init__.py:34: UserWarning: \u001b[33m\n",
      "C++ implementation not available. Using pure Python.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "class TTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)  # 输出: 32x32x16\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # 输出: 16x16x16\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # 输出: 16x16x32\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # 输出: 8x8x32\n",
    "        self.conv3 = nn.Conv2d(32, 16, kernel_size=3, padding=1)  # 输出: 8x8x16\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # 输出: 4x4x16 => 展平后是 4*4*16 = 256 个元素\n",
    "        self.conv4 = nn.Conv2d(16, 64, kernel_size=3, padding=1)  # 输出: 4x4x64\n",
    "        \n",
    "        import torchtt as tntt\n",
    "        self.ttl1 = tntt.nn.LinearLayerTT([4,4,4,4,4], [3,3,3,3,3], [1,3,3,3,3,1])\n",
    "        self.fc2 = nn.Linear(243, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        # 重塑为TT层所需的形状\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, 4, 4, 4, 4, 4)  # 4^5 = 1024\n",
    "        x = F.relu(self.ttl1(x))\n",
    "        # 重塑回二维形状以通过全连接层\n",
    "        x = x.reshape(batch_size, -1)  # 应该是 [batch_size, 243]\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "ttnet = TTNet().to(device) #use GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Define a Loss function and optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer1 = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer2 = optim.SGD(ttnet.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Train and test the network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数：计算准确率\n",
    "def accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# 函数：计算模型的参数量\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:\n",
      "[1,  2000] net loss: 2.303\n",
      "[1,  4000] net loss: 2.233\n",
      "[1,  6000] net loss: 1.958\n",
      "[1,  8000] net loss: 1.734\n",
      "[1, 10000] net loss: 1.613\n",
      "[1, 12000] net loss: 1.517\n",
      "[1,  2000] ttnet loss: 2.279\n",
      "[1,  4000] ttnet loss: 2.008\n",
      "[1,  6000] ttnet loss: 1.747\n",
      "[1,  8000] ttnet loss: 1.614\n",
      "[1, 10000] ttnet loss: 1.525\n",
      "[1, 12000] ttnet loss: 1.435\n",
      "  Net - Loss: 1.8752, Acc: 0.4423, Time: 72.39s\n",
      "  TTNet - Loss: 1.7546, Acc: 0.4929, Time: 106.60s\n",
      "  Speed Ratio: 0.68x\n",
      "----------------------------------------\n",
      "Epoch 2/5:\n",
      "[2,  2000] net loss: 1.432\n",
      "[2,  4000] net loss: 1.378\n",
      "[2,  6000] net loss: 1.351\n",
      "[2,  8000] net loss: 1.314\n",
      "[2, 10000] net loss: 1.263\n",
      "[2, 12000] net loss: 1.226\n",
      "[2,  2000] ttnet loss: 1.367\n",
      "[2,  4000] ttnet loss: 1.315\n",
      "[2,  6000] ttnet loss: 1.284\n",
      "[2,  8000] ttnet loss: 1.238\n",
      "[2, 10000] ttnet loss: 1.215\n",
      "[2, 12000] ttnet loss: 1.191\n",
      "  Net - Loss: 1.3218, Acc: 0.5744, Time: 75.93s\n",
      "  TTNet - Loss: 1.2640, Acc: 0.5911, Time: 86.44s\n",
      "  Speed Ratio: 0.88x\n",
      "----------------------------------------\n",
      "Epoch 3/5:\n",
      "[3,  2000] net loss: 1.137\n",
      "[3,  4000] net loss: 1.116\n",
      "[3,  6000] net loss: 1.104\n",
      "[3,  8000] net loss: 1.089\n",
      "[3, 10000] net loss: 1.077\n",
      "[3, 12000] net loss: 1.051\n",
      "[3,  2000] ttnet loss: 1.109\n",
      "[3,  4000] ttnet loss: 1.118\n",
      "[3,  6000] ttnet loss: 1.085\n",
      "[3,  8000] ttnet loss: 1.068\n",
      "[3, 10000] ttnet loss: 1.065\n",
      "[3, 12000] ttnet loss: 1.044\n",
      "  Net - Loss: 1.0943, Acc: 0.6307, Time: 74.99s\n",
      "  TTNet - Loss: 1.0788, Acc: 0.6395, Time: 105.58s\n",
      "  Speed Ratio: 0.71x\n",
      "----------------------------------------\n",
      "Epoch 4/5:\n",
      "[4,  2000] net loss: 0.946\n",
      "[4,  4000] net loss: 0.967\n",
      "[4,  6000] net loss: 0.974\n",
      "[4,  8000] net loss: 0.956\n",
      "[4, 10000] net loss: 0.951\n",
      "[4, 12000] net loss: 0.961\n",
      "[4,  2000] ttnet loss: 0.989\n",
      "[4,  4000] ttnet loss: 0.989\n",
      "[4,  6000] ttnet loss: 0.979\n",
      "[4,  8000] ttnet loss: 0.977\n",
      "[4, 10000] ttnet loss: 0.965\n",
      "[4, 12000] ttnet loss: 0.950\n",
      "  Net - Loss: 0.9585, Acc: 0.6512, Time: 65.08s\n",
      "  TTNet - Loss: 0.9744, Acc: 0.6445, Time: 105.66s\n",
      "  Speed Ratio: 0.62x\n",
      "----------------------------------------\n",
      "Epoch 5/5:\n",
      "[5,  2000] net loss: 0.853\n",
      "[5,  4000] net loss: 0.865\n",
      "[5,  6000] net loss: 0.850\n",
      "[5,  8000] net loss: 0.900\n",
      "[5, 10000] net loss: 0.842\n",
      "[5, 12000] net loss: 0.853\n",
      "[5,  2000] ttnet loss: 0.922\n",
      "[5,  4000] ttnet loss: 0.925\n",
      "[5,  6000] ttnet loss: 0.897\n",
      "[5,  8000] ttnet loss: 0.884\n",
      "[5, 10000] ttnet loss: 0.905\n",
      "[5, 12000] ttnet loss: 0.884\n",
      "  Net - Loss: 0.8615, Acc: 0.6784, Time: 75.50s\n",
      "  TTNet - Loss: 0.9047, Acc: 0.6766, Time: 108.53s\n",
      "  Speed Ratio: 0.70x\n",
      "----------------------------------------\n",
      "Finished Training\n",
      "Model Parameters:\n",
      "  Net: 289,413 parameters\n",
      "  TTNet: 40,977 parameters\n",
      "  Compression Ratio: 7.06x\n",
      "Training Time:\n",
      "  Net: 363.89s (avg: 72.78s/epoch)\n",
      "  TTNet: 512.82s (avg: 102.56s/epoch)\n",
      "  Speed Ratio: 0.71x\n",
      "Final Accuracy:\n",
      "  Net: 0.6784\n",
      "  TTNet: 0.6766\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 5\n",
    "# 初始化 TensorBoard SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# 用于存储每个 epoch 的准确率和损失数据\n",
    "accuracy_history_net = []\n",
    "accuracy_history_ttnet = []\n",
    "loss_history_net = []\n",
    "loss_history_ttnet = []\n",
    "\n",
    "# 用于存储训练时间\n",
    "time_history_net = []\n",
    "time_history_ttnet = []\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    \n",
    "    # ==================== 训练第一个模型(net) ====================\n",
    "    start_time_net = time.time()  # 开始时间\n",
    "    \n",
    "    running_loss_net = 0.0\n",
    "    total_loss_net = 0.0\n",
    "    batch_count = 0\n",
    "    net.train()  # 设置为训练模式\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # 获取输入数据\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        # 梯度清零\n",
    "        optimizer1.zero_grad()\n",
    "        \n",
    "        # 前向传播 + 反向传播 + 优化\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "        # 统计损失\n",
    "        running_loss_net += loss.item()\n",
    "        total_loss_net += loss.item()\n",
    "        batch_count += 1\n",
    "        \n",
    "        if i % 2000 == 1999:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] net loss: {running_loss_net / 2000:.3f}')\n",
    "            writer.add_scalar('Training_Loss_Detail', running_loss_net / 2000, epoch * len(trainloader) + i)\n",
    "            running_loss_net = 0.0\n",
    "    \n",
    "    # 计算训练时间\n",
    "    end_time_net = time.time()\n",
    "    epoch_time_net = end_time_net - start_time_net\n",
    "    time_history_net.append(epoch_time_net)\n",
    "    writer.add_scalar('Training_Time/net', epoch_time_net, epoch)\n",
    "    \n",
    "    # 计算并记录平均每个epoch的损失\n",
    "    epoch_loss_net = total_loss_net / batch_count\n",
    "    loss_history_net.append(epoch_loss_net)\n",
    "    writer.add_scalar('Loss/net', epoch_loss_net, epoch)\n",
    "    \n",
    "    # ==================== 训练第二个模型(ttnet) ====================\n",
    "    start_time_ttnet = time.time()  # 开始时间\n",
    "    \n",
    "    running_loss_ttnet = 0.0\n",
    "    total_loss_ttnet = 0.0\n",
    "    batch_count = 0\n",
    "    ttnet.train()  # 设置为训练模式\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # 获取输入数据\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        # 梯度清零\n",
    "        optimizer2.zero_grad()\n",
    "        \n",
    "        # 前向传播 + 反向传播 + 优化\n",
    "        outputs = ttnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "        \n",
    "        # 统计损失\n",
    "        running_loss_ttnet += loss.item()\n",
    "        total_loss_ttnet += loss.item()\n",
    "        batch_count += 1\n",
    "        \n",
    "        if i % 2000 == 1999:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] ttnet loss: {running_loss_ttnet / 2000:.3f}')\n",
    "            writer.add_scalar('Training_Loss_Detail', running_loss_ttnet / 2000, epoch * len(trainloader) + i)\n",
    "            running_loss_ttnet = 0.0\n",
    "    \n",
    "    # 计算训练时间\n",
    "    end_time_ttnet = time.time()\n",
    "    epoch_time_ttnet = end_time_ttnet - start_time_ttnet\n",
    "    time_history_ttnet.append(epoch_time_ttnet)\n",
    "    writer.add_scalar('Training_Time/ttnet', epoch_time_ttnet, epoch)\n",
    "    \n",
    "    # 计算并记录平均每个epoch的损失\n",
    "    epoch_loss_ttnet = total_loss_ttnet / batch_count\n",
    "    loss_history_ttnet.append(epoch_loss_ttnet)\n",
    "    writer.add_scalar('Loss/ttnet', epoch_loss_ttnet, epoch)\n",
    "    \n",
    "    # ==================== 评估模型 ====================\n",
    "    net.eval()  # 设置为评估模式\n",
    "    ttnet.eval()\n",
    "    \n",
    "    # 计算准确率\n",
    "    acc_net = accuracy(net, testloader, device)\n",
    "    acc_ttnet = accuracy(ttnet, testloader, device)\n",
    "    \n",
    "    # 记录准确率历史\n",
    "    accuracy_history_net.append(acc_net)\n",
    "    accuracy_history_ttnet.append(acc_ttnet)\n",
    "    \n",
    "    # 在同一个图表中记录两个模型的准确率\n",
    "    writer.add_scalar('Accuracy/net', acc_net, epoch)\n",
    "    writer.add_scalar('Accuracy/ttnet', acc_ttnet, epoch)\n",
    "    \n",
    "    # 打印当前 epoch 的结果\n",
    "    print(f'  Net - Loss: {epoch_loss_net:.4f}, Acc: {acc_net:.4f}, Time: {epoch_time_net:.2f}s')\n",
    "    print(f'  TTNet - Loss: {epoch_loss_ttnet:.4f}, Acc: {acc_ttnet:.4f}, Time: {epoch_time_ttnet:.2f}s')\n",
    "    print(f'  Speed Ratio: {epoch_time_net/epoch_time_ttnet:.2f}x')\n",
    "    print('-'*40)\n",
    "    \n",
    "    # 记录速度比（net训练时间/ttnet训练时间）\n",
    "    speed_ratio = epoch_time_net / epoch_time_ttnet\n",
    "    writer.add_scalar('Speed_Ratio/net_vs_ttnet', speed_ratio, epoch)\n",
    "\n",
    "# ==================== 训练结束，计算总结统计 ====================\n",
    "print('Finished Training')\n",
    "\n",
    "# 计算总训练时间\n",
    "total_time_net = sum(time_history_net)\n",
    "total_time_ttnet = sum(time_history_ttnet)\n",
    "avg_time_net = total_time_net / num_epochs\n",
    "avg_time_ttnet = total_time_ttnet / num_epochs\n",
    "overall_speed_ratio = total_time_net / total_time_ttnet\n",
    "\n",
    "# 计算最终的模型参数量和压缩率\n",
    "params_net = count_parameters(net)\n",
    "params_ttnet = count_parameters(ttnet)\n",
    "compression_ratio = params_net / params_ttnet\n",
    "\n",
    "# 记录总体统计数据\n",
    "writer.add_scalar('Training_Time_Total/net', total_time_net, 0)\n",
    "writer.add_scalar('Training_Time_Total/ttnet', total_time_ttnet, 0)\n",
    "writer.add_scalar('Training_Time_Average/net', avg_time_net, 0)\n",
    "writer.add_scalar('Training_Time_Average/ttnet', avg_time_ttnet, 0)\n",
    "writer.add_scalar('Speed_Ratio/overall', overall_speed_ratio, 0)\n",
    "writer.add_scalar('Parameters/net', params_net, 0)\n",
    "writer.add_scalar('Parameters/ttnet', params_ttnet, 0)\n",
    "writer.add_scalar('Parameters/compression_ratio', compression_ratio, 0)\n",
    "\n",
    "# 打印最终结果\n",
    "print(f'Model Parameters:')\n",
    "print(f'  Net: {params_net:,} parameters')\n",
    "print(f'  TTNet: {params_ttnet:,} parameters')\n",
    "print(f'  Compression Ratio: {compression_ratio:.2f}x')\n",
    "\n",
    "print(f'Training Time:')\n",
    "print(f'  Net: {total_time_net:.2f}s (avg: {avg_time_net:.2f}s/epoch)')\n",
    "print(f'  TTNet: {total_time_ttnet:.2f}s (avg: {avg_time_ttnet:.2f}s/epoch)')\n",
    "print(f'  Speed Ratio: {overall_speed_ratio:.2f}x')\n",
    "\n",
    "print(f'Final Accuracy:')\n",
    "print(f'  Net: {accuracy_history_net[-1]:.4f}')\n",
    "print(f'  TTNet: {accuracy_history_ttnet[-1]:.4f}')\n",
    "\n",
    "images = torch.rand([1, 3, 32, 32], dtype=torch.float32).to(device)\n",
    "writer.add_graph(net, images)\n",
    "writer.add_graph(ttnet, images)\n",
    "# 关闭 TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH1 = './cifar_net.pth'\n",
    "PATH2 = './cifar_ttnet.pth'\n",
    "torch.save(net.state_dict(), PATH1)\n",
    "torch.save(ttnet.state_dict(), PATH2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Reload the network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Net()\n",
    "# net.load_state_dict(torch.load(PATH1, weights_only=True))\n",
    "# ttnet = TTNet()\n",
    "# ttnet.load_state_dict(torch.load(PATH2, weights_only=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
